{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n@Author: Vighnesh Harish Bilgi\\n@Date: 2022-11-25\\n@Last Modified by: Vighnesh Harish Bilgi\\n@Last Modified time: 2022-11-25\\n@Title : Genrerate random records and upload them to dynamoDB\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "@Author: Vighnesh Harish Bilgi\n",
    "@Date: 2022-11-25\n",
    "@Last Modified by: Vighnesh Harish Bilgi\n",
    "@Last Modified time: 2022-11-25\n",
    "@Title : Genrerate random records and upload them to dynamoDB\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import names\n",
    "import random\n",
    "import time\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = os.environ.get('test1_access_key')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = os.environ.get('test1_secret_access_key')\n",
    "TABLE_NAME = 'Auto_Load_Table'\n",
    "BUCKET_NAME = 'auto-load-bucket'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom fucntions to create dynamoDB Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_dynamoDB():\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        To connect to AWS DynamoDB service.\n",
    "    Parameter:\n",
    "        No parameters\n",
    "    Return:\n",
    "        ServiceResource dyDB\n",
    "\n",
    "    \"\"\"\n",
    "    dyDB =  boto3.resource('dynamodb')\n",
    "    return dyDB\n",
    "\n",
    "def create_items(table,dataset_records):\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        To create items in a table of DynamoDB.\n",
    "    Parameter:\n",
    "        dynamodb.table table\n",
    "    Return:\n",
    "        No values returned.\n",
    "\n",
    "    \"\"\"\n",
    "    table.put_item(\n",
    "        Item={\n",
    "                'id': dataset_records[0],\n",
    "                'name': dataset_records[1]\n",
    "            }\n",
    "    )\n",
    "\n",
    "def create_table(dyDB,table_name):\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        To create a dynamoDB table if it doen't exist\n",
    "    Parameter:\n",
    "        ServiceResource dyDB\n",
    "    Return:\n",
    "        no value returned\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    dynamodb_client = boto3.client('dynamodb')\n",
    "    existing_tables = dynamodb_client.list_tables()['TableNames']\n",
    "\n",
    "    if table_name not in existing_tables:\n",
    "        # Create the DynamoDB table.\n",
    "        dyDB.create_table(\n",
    "            TableName=table_name,\n",
    "            KeySchema=[\n",
    "                {\n",
    "                    'AttributeName': 'id',\n",
    "                    'KeyType': 'HASH'\n",
    "                },\n",
    "                {\n",
    "                    'AttributeName': 'name',\n",
    "                    'KeyType': 'RANGE'\n",
    "                }\n",
    "            ],\n",
    "            AttributeDefinitions=[\n",
    "                {\n",
    "                    'AttributeName': 'id',\n",
    "                    'AttributeType': 'N'\n",
    "                },\n",
    "                {\n",
    "                    'AttributeName': 'name',\n",
    "                    'AttributeType': 'S'\n",
    "                },\n",
    "            ],\n",
    "            ProvisionedThroughput={\n",
    "                'ReadCapacityUnits': 5,\n",
    "                'WriteCapacityUnits': 5\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom fucntions to connect to S3 resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_s3_client():\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        To connect to AWS S3 service.\n",
    "    Parameter:\n",
    "        No parameters\n",
    "    Return:\n",
    "        ServiceResource s3\n",
    "    \"\"\"\n",
    "    # s3 =  boto3.resource('s3')\n",
    "    client = boto3.client(\"s3\")\n",
    "    return client\n",
    "\n",
    "\n",
    "def connect_to_s3_resource():\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        To connect to AWS S3 service through an IAM user.\n",
    "    Parameter:\n",
    "        No parameters\n",
    "    Return:\n",
    "        ServiceResource s3\n",
    "    \"\"\"\n",
    "    s3 =  boto3.resource(service_name = 's3')\n",
    "    return s3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom fucntions to create upload data to DynamoDB and send each item from DynamoDB as .csv file to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamo_to_s3(table):\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        Fetching each item from dynamoDB Table 'table' as .csv file and uploading them to S3 bucket\n",
    "    Parameter:\n",
    "        dynamodb.table table\n",
    "    Return:\n",
    "        No values returned\n",
    "\n",
    "    \"\"\"\n",
    "    s3 = connect_to_s3_resource()\n",
    "    \n",
    "    table_details = table.scan()\n",
    "    table_items = table_details['Items']\n",
    "\n",
    "    print(f\"Displaying objects in {BUCKET_NAME}:\")\n",
    "\n",
    "    count = 1\n",
    "    for item in table_items:\n",
    "\n",
    "        file_name = f'record#{count}.csv'\n",
    "\n",
    "        csv_buffer = StringIO()\n",
    "        df = pd.DataFrame(item, index=[0]) \n",
    "        df.to_csv(csv_buffer, index= False)\n",
    "        s3.Object(BUCKET_NAME, f'data-output/{file_name}').put(Body=csv_buffer.getvalue())\n",
    "\n",
    "        print(f\"{file_name} is uploaded to S3 Bucket '{BUCKET_NAME}'\")\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        # wait for 30 seconds before next iteration\n",
    "        time.sleep(30)\n",
    "\n",
    "\n",
    "def generate_and_upload(table,n):\n",
    "    \"\"\"\n",
    "\n",
    "    Description:\n",
    "        Checking if 'id' is duplicate from list record before uploading them as an item to DynamoDB Table\n",
    "    Parameter:\n",
    "        dynamodb.table table,\n",
    "        integer n\n",
    "    Return:\n",
    "        No values returned\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    check_id = []\n",
    "    count = 1\n",
    "    for i in range(n):\n",
    "        record = [random.randint(100,999), names.get_full_name()]\n",
    "        \n",
    "        if record[0] not in check_id:\n",
    "\n",
    "            check_id.append(record[0])\n",
    "            print(f\"Record #{i+1} data : {record}\")\n",
    "            create_items(table,record)\n",
    "\n",
    "        else:\n",
    "            print(f\"Record id : {record[0]} already exists! \")    \n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        # wait for 5 seconds before next iteration\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing all bucket names to verify if - auto-load-bucket is created:\n",
      "auto-load-bucket\n",
      "aws-cloudtrail-logs-949401335332-4af97cdf\n",
      "aws-cloudtrail-logs-949401335332-a2ad74b3\n"
     ]
    }
   ],
   "source": [
    "s3 = connect_to_s3_resource()\n",
    "client = connect_to_s3_client()\n",
    "\n",
    "# creating new bucket\n",
    "client.create_bucket(Bucket = BUCKET_NAME)\n",
    "print(f\"Printing all bucket names to verify if - {BUCKET_NAME} is created:\")\n",
    "for bucket in s3.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Send data to DynamoDB Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime creation of Table : 2022-12-07 11:18:38.846000+05:30\n",
      "Record #1 data : [538, 'Robert Herrera']\n",
      "Record #2 data : [942, 'William Thompson']\n",
      "Record #3 data : [401, 'Clifford Tyler']\n",
      "Record #4 data : [772, 'Sandra Charlot']\n",
      "Record #5 data : [548, 'Lauren Self']\n",
      "Record #6 data : [119, 'Keri Ryan']\n",
      "Record #7 data : [489, 'Debra Hoover']\n",
      "Record #8 data : [662, 'Robert Grasso']\n",
      "Record #9 data : [411, 'Sharon Petrich']\n",
      "Record #10 data : [604, 'Marcos Clyde']\n",
      "Record #11 data : [316, 'Josephine Everett']\n",
      "Record #12 data : [779, 'Justin Beck']\n",
      "Record #13 data : [597, 'Candice Jethva']\n",
      "Record #14 data : [267, 'Brian Deboer']\n",
      "Record #15 data : [456, 'Margaret Lavalley']\n",
      "Record #16 data : [394, 'Billie Stillman']\n",
      "Record #17 data : [449, 'Jennifer Padilla']\n",
      "Record #18 data : [727, 'Jeffery Smith']\n",
      "Record #19 data : [500, 'Carolyn Jacobson']\n",
      "Record #20 data : [577, 'Brandon Hernandez']\n",
      "Record #21 data : [266, 'Kellie Williams']\n",
      "Record #22 data : [830, 'Shirley Nooe']\n",
      "Record #23 data : [757, 'Myron Hines']\n",
      "Record #24 data : [395, 'Norma Daniels']\n",
      "Record #25 data : [962, 'Thomas Nightingale']\n",
      "Record #26 data : [209, 'Magdalena Hughes']\n",
      "Record #27 data : [698, 'Peter Ball']\n",
      "Record #28 data : [454, 'Estelle Pruitt']\n",
      "Record id : 267 already exists! \n",
      "Record #30 data : [897, 'Linda Lewis']\n",
      "Record #31 data : [775, 'Richard Wint']\n",
      "Record #32 data : [544, 'Mary Oleson']\n",
      "Record #33 data : [864, 'Amanda Galloway']\n",
      "Record #34 data : [678, 'Heather Vitale']\n",
      "Record id : 267 already exists! \n",
      "Record #36 data : [255, 'Delma Gose']\n",
      "Record #37 data : [571, 'Julian Reutzel']\n",
      "Record #38 data : [971, 'Richard Citron']\n",
      "Record #39 data : [844, 'Herbert Matthew']\n",
      "Record #40 data : [205, 'Eric Thompson']\n",
      "Record #41 data : [835, 'Maxine Rameriez']\n",
      "Record #42 data : [885, 'Kenneth Jones']\n",
      "Record #43 data : [472, 'Christopher Cobb']\n",
      "Record #44 data : [628, 'Brad Nottingham']\n",
      "Record #45 data : [963, 'Vincent Skerl']\n",
      "Record #46 data : [943, 'Terrance Richardson']\n",
      "Record #47 data : [966, 'Rebecca Madril']\n",
      "Record #48 data : [989, 'Melissa Lay']\n",
      "Record #49 data : [795, 'Margaret Giese']\n",
      "Record id : 538 already exists! \n",
      "Record #51 data : [945, 'Jackie Grant']\n",
      "Record #52 data : [164, 'Helen Shehan']\n",
      "Record #53 data : [169, 'Alice Andrews']\n",
      "Record #54 data : [167, 'Pedro White']\n",
      "Record #55 data : [671, 'Adam Baldwin']\n",
      "Record #56 data : [883, 'Byron Gould']\n",
      "Record #57 data : [173, 'Floyd Wilkins']\n",
      "Record #58 data : [492, 'Carla Villanueva']\n",
      "Record #59 data : [426, 'Vickey Williamson']\n",
      "Record #60 data : [784, 'Robert Scheller']\n",
      "Record #61 data : [766, 'Joyce Cox']\n",
      "Record #62 data : [733, 'William Raley']\n",
      "Record id : 698 already exists! \n",
      "Record #64 data : [494, 'Linda Breault']\n",
      "Record #65 data : [122, 'Sue Berri']\n",
      "Record #66 data : [924, 'Donna Robinson']\n",
      "Record #67 data : [565, 'Janice Homan']\n",
      "Record #68 data : [954, 'David Armitage']\n",
      "Record #69 data : [711, 'Brenda Benson']\n",
      "Record #70 data : [297, 'William Houghton']\n",
      "Record #71 data : [588, 'Nina Hoffman']\n",
      "Record #72 data : [348, 'Keith Hughes']\n",
      "Record #73 data : [867, 'Alberto Rutkowski']\n",
      "Record #74 data : [983, 'Debra Cruz']\n",
      "Record #75 data : [309, 'Martha Shaffer']\n",
      "Record #76 data : [886, 'Vivian Caballero']\n",
      "Record #77 data : [212, 'Phyllis Robinson']\n",
      "Record #78 data : [977, 'Judy Strickland']\n",
      "Record id : 963 already exists! \n",
      "Record #80 data : [509, 'Marshall Elder']\n",
      "Record #81 data : [898, 'Michael Albert']\n",
      "Record id : 772 already exists! \n",
      "Record #83 data : [900, 'John Gore']\n",
      "Record #84 data : [587, 'Monique Blodgett']\n",
      "Record #85 data : [817, 'Christine Dunn']\n",
      "Record #86 data : [585, 'Robert Greene']\n",
      "Record #87 data : [193, 'Mary Banerjee']\n",
      "Record #88 data : [459, 'William Abney']\n",
      "Record #89 data : [837, 'Raymond Appling']\n",
      "Record #90 data : [931, 'Carla Sheppard']\n",
      "Record #91 data : [804, 'Christopher Thomas']\n",
      "Record #92 data : [670, 'Kathryn Davis']\n",
      "Record #93 data : [398, 'Pamela Haskell']\n",
      "Record #94 data : [949, 'Lillie Delsignore']\n",
      "Record #95 data : [507, 'Cindy Sullivan']\n",
      "Record #96 data : [470, 'Mary Dunton']\n",
      "Record #97 data : [162, 'Krystina Tuck']\n",
      "Record #98 data : [651, 'Kevin Brown']\n",
      "Record #99 data : [891, 'Mildred Archambault']\n",
      "Record id : 698 already exists! \n"
     ]
    }
   ],
   "source": [
    "dyDB = connect_to_dynamoDB()\n",
    "\n",
    "create_table(dyDB,TABLE_NAME)\n",
    "\n",
    "table = dyDB.Table(TABLE_NAME)\n",
    "table.wait_until_exists()\n",
    "print(f\"DateTime creation of Table : {table.creation_date_time}\")\n",
    "\n",
    "n = int(input(\"Enter number of times to run loop:\"))\n",
    "\n",
    "generate_and_upload(table,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Send items from DynamoDB Table as .csv file to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying objects in auto-load-bucket:\n",
      "record#1.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#2.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#3.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#4.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#5.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#6.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#7.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#8.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#9.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#10.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#11.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#12.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#13.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#14.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#15.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#16.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#17.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#18.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#19.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#20.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#21.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#22.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#23.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#24.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#25.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#26.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#27.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#28.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#29.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#30.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#31.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#32.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#33.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#34.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#35.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#36.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#37.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#38.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#39.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#40.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#41.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#42.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#43.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#44.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#45.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#46.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#47.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#48.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#49.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#50.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#51.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#52.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#53.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#54.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#55.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#56.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#57.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#58.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#59.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#60.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#61.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#62.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#63.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#64.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#65.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#66.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#67.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#68.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#69.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#70.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#71.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#72.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#73.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#74.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#75.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#76.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#77.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#78.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#79.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#80.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#81.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#82.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#83.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#84.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#85.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#86.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#87.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#88.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#89.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#90.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#91.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#92.csv is uploaded to S3 Bucket 'auto-load-bucket'\n",
      "record#93.csv is uploaded to S3 Bucket 'auto-load-bucket'\n"
     ]
    }
   ],
   "source": [
    "table = dyDB.Table(TABLE_NAME)\n",
    "dynamo_to_s3(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
